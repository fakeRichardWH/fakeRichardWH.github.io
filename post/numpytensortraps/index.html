<!DOCTYPE html>
<html lang="en-us">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
        <title>Pytorch中view方法的坑 - fakeRichardWH的个人网站</title><meta name="Description" content="这是我的全新 Hugo 网站"><meta property="og:title" content="Pytorch中view方法的坑" />
<meta property="og:description" content="pytorch View视图 你咋就这么特别呢？ 查看Pytorch官方文档: PyTorch allows a tensor to be a View(视图) of an existing tensor.View(视图) tensor shares the same underlying data with" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/post/numpytensortraps/" /><meta property="og:image" content="http://localhost:1313/logo.png"/><meta property="article:section" content="post" />
<meta property="article:published_time" content="2021-12-11T14:41:12+08:00" />
<meta property="article:modified_time" content="2021-12-11T14:41:12+08:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="http://localhost:1313/logo.png"/>

<meta name="twitter:title" content="Pytorch中view方法的坑"/>
<meta name="twitter:description" content="pytorch View视图 你咋就这么特别呢？ 查看Pytorch官方文档: PyTorch allows a tensor to be a View(视图) of an existing tensor.View(视图) tensor shares the same underlying data with"/>
<meta name="application-name" content="LoveIt">
<meta name="apple-mobile-web-app-title" content="LoveIt"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="icon" href="static/images/favicon-32x32.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="http://localhost:1313/post/numpytensortraps/" /><link rel="prev" href="http://localhost:1313/post/deeplearning-cnn-fundamentals1/" /><link rel="next" href="http://localhost:1313/post/removeduplicatesinarray/" /><link rel="stylesheet" href="/lib/normalize/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "Pytorch中view方法的坑",
        "inLanguage": "en-us",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "http:\/\/localhost:1313\/post\/numpytensortraps\/"
        },"genre": "post","keywords": "pytorch, 踩坑","wordcount":  986 ,
        "url": "http:\/\/localhost:1313\/post\/numpytensortraps\/","datePublished": "2021-12-11T14:41:12+08:00","dateModified": "2021-12-11T14:41:12+08:00","publisher": {
            "@type": "Organization",
            "name": ""},"author": {
                "@type": "Person",
                "name": "RichardWWHH"
            },"description": ""
    }
    </script></head>
    <body header-desktop="fixed" header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('light' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'light' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="fakeRichardWH的个人网站"></a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/post/"> 文章/articles </a><a class="menu-item" href="/tags/"> 标签/tags </a><a class="menu-item" href="/categories/"> 分类/categories </a><a class="menu-item" href="/about/"> 关于我/about me </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="Search">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="Clear">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw"></i>
                </a>
            </div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="fakeRichardWH的个人网站"></a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="Search titles or contents..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="Search">
                            <i class="fas fa-search fa-fw"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="Clear">
                            <i class="fas fa-times-circle fa-fw"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        Cancel
                    </a>
                </div><a class="menu-item" href="/post/" title="">文章/articles</a><a class="menu-item" href="/tags/" title="">标签/tags</a><a class="menu-item" href="/categories/" title="">分类/categories</a><a class="menu-item" href="/about/" title="">关于我/about me</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw"></i>
            </a></div>
    </div>
</header>
<div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div>
<main class="main">
                <div class="container"><div class="page single special"><h1 class="single-title animated pulse faster">Pytorch中view方法的坑</h1><div class="content" id="content"><h1 id="pytorch-view视图">pytorch View视图</h1>
<h2 id="你咋就这么特别呢">你咋就这么特别呢？</h2>
<p>查看Pytorch官方文档:</p>
<blockquote>
<p>PyTorch allows a tensor to be a <code>View(视图)</code> of an existing tensor.<code>View(视图)</code> tensor shares the same underlying data with its base tensor. Supporting <code>View(视图)</code> avoids explicit data copy, thus allows us to do fast and memory efficient reshaping, slicing and element-wise operations.
也就是说 视图类型的tensor 本身 不是显式的拷贝了数据，而是使用了它的一个引用。</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">&gt;&gt;&gt;</span> <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span> 
<span class="o">&gt;&gt;&gt;</span> <span class="n">t</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span> <span class="o">==</span> <span class="n">b</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span> <span class="c1"># True &#39;t&#39; 和 &#39;b&#39; 用的是同一块内存</span>

<span class="c1"># 但是 b 的 shape 属性是改变了的</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">t</span><span class="o">.</span><span class="n">shape</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mf">0.4473</span><span class="p">,</span> <span class="mf">0.3079</span><span class="p">,</span> <span class="mf">0.5223</span><span class="p">,</span> <span class="mf">0.2924</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.9936</span><span class="p">,</span> <span class="mf">0.9032</span><span class="p">,</span> <span class="mf">0.7414</span><span class="p">,</span> <span class="mf">0.6115</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.0904</span><span class="p">,</span> <span class="mf">0.6525</span><span class="p">,</span> <span class="mf">0.6720</span><span class="p">,</span> <span class="mf">0.8673</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.0829</span><span class="p">,</span> <span class="mf">0.1356</span><span class="p">,</span> <span class="mf">0.9617</span><span class="p">,</span> <span class="mf">0.2030</span><span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mf">0.4473</span><span class="p">,</span> <span class="mf">0.3079</span><span class="p">,</span> <span class="mf">0.5223</span><span class="p">,</span> <span class="mf">0.2924</span><span class="p">,</span> <span class="mf">0.9936</span><span class="p">,</span> <span class="mf">0.9032</span><span class="p">,</span> <span class="mf">0.7414</span><span class="p">,</span> <span class="mf">0.6115</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.0904</span><span class="p">,</span> <span class="mf">0.6525</span><span class="p">,</span> <span class="mf">0.6720</span><span class="p">,</span> <span class="mf">0.8673</span><span class="p">,</span> <span class="mf">0.0829</span><span class="p">,</span> <span class="mf">0.1356</span><span class="p">,</span> <span class="mf">0.9617</span><span class="p">,</span> <span class="mf">0.2030</span><span class="p">]])</span>
</code></pre></td></tr></table>
</div>
</div><p>由于共用一个地址空间，修改 b 也会同时修改 t</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span><span class="lnt">9
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="o">&gt;&gt;&gt;</span> <span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mf">1.0000</span><span class="p">,</span> <span class="mf">0.3079</span><span class="p">,</span> <span class="mf">0.5223</span><span class="p">,</span> <span class="mf">0.2924</span><span class="p">,</span> <span class="mf">0.9936</span><span class="p">,</span> <span class="mf">0.9032</span><span class="p">,</span> <span class="mf">0.7414</span><span class="p">,</span> <span class="mf">0.6115</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.0904</span><span class="p">,</span> <span class="mf">0.6525</span><span class="p">,</span> <span class="mf">0.6720</span><span class="p">,</span> <span class="mf">0.8673</span><span class="p">,</span> <span class="mf">0.0829</span><span class="p">,</span> <span class="mf">0.1356</span><span class="p">,</span> <span class="mf">0.9617</span><span class="p">,</span> <span class="mf">0.2030</span><span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mf">1.0000</span><span class="p">,</span> <span class="mf">0.3079</span><span class="p">,</span> <span class="mf">0.5223</span><span class="p">,</span> <span class="mf">0.2924</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.9936</span><span class="p">,</span> <span class="mf">0.9032</span><span class="p">,</span> <span class="mf">0.7414</span><span class="p">,</span> <span class="mf">0.6115</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.0904</span><span class="p">,</span> <span class="mf">0.6525</span><span class="p">,</span> <span class="mf">0.6720</span><span class="p">,</span> <span class="mf">0.8673</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.0829</span><span class="p">,</span> <span class="mf">0.1356</span><span class="p">,</span> <span class="mf">0.9617</span><span class="p">,</span> <span class="mf">0.2030</span><span class="p">]])</span>
</code></pre></td></tr></table>
</div>
</div><h2 id="与view类似的方法">与view()类似的方法</h2>
<blockquote>
<p>Typically a PyTorch op returns a new tensor as output, e.g. add(). But in case of view ops, outputs are views of input tensors to avoid unncessary data copy. No data movement occurs when creating a view, view tensor just changes the way it interprets the same data.
Taking a view of contiguous tensor could potentially produce a non-contiguous tensor. Users should be pay additional attention as contiguity might have implicit performance impact. transpose() is a common example.</p>
</blockquote>
<p>可以看到一般来说一个Pytorch操作会返回一个新的tensor作为输出，也就是在一个新内存空间上申请了一个位置，例如add()方法，但是对于view的这种方式，一般输出都会避免不必要的数据深拷贝,仅仅是改变数据的读取顺序。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">## 下面的代码中的变量与前面的一致</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">id</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="o">==</span> <span class="nb">id</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> 
<span class="kc">False</span>
</code></pre></td></tr></table>
</div>
</div><p><strong>需要注意</strong>的是：<code>view()</code>和<code>reshape()</code>最大的区别就是当你用<code>view()</code>查看一个连续的tensor的时候会产生不连续的tensor</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span><span class="lnt">6
</span><span class="lnt">7
</span><span class="lnt">8
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="c1">## 下面的代码中的变量与前面的一致</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">t</span> <span class="o">=</span> <span class="n">d</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># `t`是d的一个view，没有发生地址变化</span>
<span class="c1"># 但是view tensor本身是不连续的</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">t</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">()</span>
<span class="kc">False</span>
<span class="c1"># 如果想要得到的 view tensor(视图张量)也是连续的</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">c</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
</code></pre></td></tr></table>
</div>
</div><p>下面列举一些常见的方法也是类似View视图的操作，并且也具有刚才的几个坑</p>
<ul>
<li>tensor[0, 2:, 1:7:2]返回的也是tensor的一个视图</li>
<li>unflatten()</li>
<li>unfold()</li>
<li>unsqueeze()</li>
<li>squeeze()</li>
<li>detach()
当通过索引来访问tensor，Pytorch和Numpy的机制类似，基础的索引就只是返回（views）视图（例如:通过索引赋值都是在原地址上修改），高级的索引操作返回的是数据的深拷贝。</li>
</ul>
<h2 id="注意区分view-和-reshape">注意区分<code>view()</code> 和 <code>reshape()</code></h2>
<p>reshape(), reshape_as() and flatten() 可以返回一个视图 或者 是一个新tensor
contiguous() 返回它自身 当且仅当输入tensor已经是连续的了，否则就会返回一个新的拷贝后的连续tensor</p>
</div><div id="comments"></div></div></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.88.1">Hugo</a> | Theme - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.2.10"><i class="far fa-kiss-wink-heart fa-fw"></i> LoveIt</a>
                </div><div class="footer-line"><i class="far fa-copyright fa-fw"></i><span itemprop="copyrightYear">2019 - 2021</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank">RichardWWHH</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/katex/copy-tex.min.css"><link rel="stylesheet" href="/lib/cookieconsent/cookieconsent.min.css"><script type="text/javascript" src="/lib/smooth-scroll/smooth-scroll.min.js"></script><script type="text/javascript" src="/lib/autocomplete/autocomplete.min.js"></script><script type="text/javascript" src="/lib/lunr/lunr.min.js"></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.js"></script><script type="text/javascript" src="/lib/clipboard/clipboard.min.js"></script><script type="text/javascript" src="/lib/sharer/sharer.min.js"></script><script type="text/javascript" src="/lib/katex/katex.min.js"></script><script type="text/javascript" src="/lib/katex/auto-render.min.js"></script><script type="text/javascript" src="/lib/katex/copy-tex.min.js"></script><script type="text/javascript" src="/lib/katex/mhchem.min.js"></script><script type="text/javascript" src="/lib/cookieconsent/cookieconsent.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":10},"comment":{},"cookieconsent":{"content":{"dismiss":"Got it!","link":"Learn more","message":"This website uses Cookies to improve your experience."},"enable":true,"palette":{"button":{"background":"#f0f0f0"},"popup":{"background":"#1aa3ff"}},"theme":"edgeless"},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"search":{"highlightTag":"em","lunrIndexURL":"/index.json","maxResultLength":10,"noResultsFound":"No results found","snippetLength":50,"type":"lunr"}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
