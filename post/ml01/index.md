# 朴素贝叶斯及监督学习模型的分类

# 朴素贝叶斯
目标: 解决的是多分类问题.
假设训练数据为$(X,Y)$,其中X为属性集合,Y为类别标记,这时候我们需要对新产生的样本x,进行类别预测,最终目标是求出最大概率:$P(y|x)$作为样本的分类.
## 边缘概率 (加法法则)
$P(X) = \sum_{Y}P(X,Y)$<br>
从直观上理解这个式子，要消除随机变量Y对联合概率的影响，那么就要固定(确定)每个Y的值，对联合概率求和，从而消除X的影响
## 条件概率与联合概率的关系 (乘法法则)
$P(X, Y) = P(Y|X)P(X) = P(Y|X)P(X)$<br>
条件概率$P(Y|X)$中由于**X**作为条件,已经被确定了那么就相当于减少（消除）了一部分的不确定性，而两个变量的联合概率分布的不确定性更大，所以需要补充的就是X本身的不确定性.

## 联合概率分布
$P(X,Y,Z) = P(X|Y,Z)P(X,Y|Z)P(Z)$
$P(X,Y,Z)$的联合概率分布，需要的是以X为变量，将Y，Z作为确定条件来降低不确定性，而补充这一不确定性需要当Z确定时的X，Y的不确定性和Z单独的不确定性
### 朴素贝叶斯分类器
## 引入条件独立性假设
$P(X, Y) = P(Y)P(X)$
此时由于X，Y是相互独立的，所以X无论确定还是不确定，对Y的条件概率无影响，从而可以将朴素贝叶斯推出来
> 条件独立性假设：**在标签一定(被确定)的基础上**，X向量或者说特征向量，各个特征之间相互独立


推导:<br>

$P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)}, ...,X^{(n)}=x^{(n)}| Y = c_{k}) = \prod_{i}^{n}P(X^{(i)}=x^{(i)}|Y)$<br>

从而：<br>

$P(X=\bf{x}|Y = c_{k}) = \prod_{i}^{n}P(X^{(i)}=x^{(i)}|Y)$


# 监督学习模型分类

## 生成式模型
生成式模型,就是对每个类别分别建立一个模型,有多少个类别就建立多少个模型.学习的是条件概率分布:P(Y|X)=P(X|Y)P(Y)/P(X),一般常见的生成式模型就是**朴素贝叶斯模型**,因为需要对每个类别分别对特征集进行划分之后,再进行统计.<br>
比如,一个信用评级模型中: 类别标签为{高, 中, 低},而属性集合包括了{房产数量,房产面积,年均收入},那么模型学习的就是三个不同的分布:$P(高|房产数量,房产面积,年均收入)$$P(中|房产数量,房产面积,年均收入)$$P(低|房产数量,房产面积,年均收入)$
而当我们得到一个未知的样本X,将其输入到三个分布中,计算出使得某个类别概率最大的就是该样本对应的类别,比如:
P(高|X) = 0.4,P(中|X)=0.5, P(低|X) = 0.1
那么我们可以认为这个样本X对应的信用评级类别就是**中**. 
### 常见的生成式模型
HMM(隐马尔可夫模型),朴素贝叶斯模型,(GMM)高斯混合模型,LDA,马尔可夫随机场模型
## 判别式模型
根据训练数据得到分类函数和划分界面,比如SVM模型就是得到多个分界面,然后直接计算条件概率.输入特征X,直接用一个模型将历史数据输入去学习拟合p(Y|X).
### 常见的判别式模型

