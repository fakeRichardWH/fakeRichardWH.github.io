# 

# 朴素贝叶斯
解决的是多分类问题
# 边缘概率
$P(X) = \sum_{Y}P(X,Y)$
从直观上理解这个式子，要消除随机变量Y对联合概率的影响，那么就要固定每个Y，对联合概率求和，从而消除X的影响
# 条件概率
$P(X, Y) = P(Y|X)P(X) = P(Y|X)P(X)$
$P(Y|X)$中由于X作为条件已经被确定了，那么减少（消除）了一部分的不确定性，而两个变量的联合不确定性更大，所以需要补充的就是X的不确定性
$P(X,Y,Z) = P(X|Y,Z)P(X,Y|Z)P(Z)$
$P(X,Y,Z)$的联合概率分布，需要的是以X为变量，将Y，Z作为确定条件来降低不确定性，而补充这一不确定性需要当Z确定时的X，Y的不确定性和Z单独的不确定性
# 引入条件独立性假设
$P(X, Y) = P(Y)P(X)$
此时由于X，Y是相互独立的，所以X无论确定还是不确定，对Y的条件概率无影响，从而可以将朴素贝叶斯推出来
> 条件独立性假设：**在标签一定(被确定)的基础上**，X向量或者说特征向量，各个特征之间相互独立

$P(X^{(1)}=x^{(1)},X^{(2)}=x^{(2)}, ...,X^{(n)}=x^{(n)}| Y = c_{k}) = \prod_{i}^{n}P(X^{(i)}=x^{(i)}|Y)$
从而：
$P(X=\bf{x}|Y = c_{k}) = \prod_{i}^{n}P(X^{(i)}=x^{(i)}|Y)$
# 贝叶斯估计
由于我们的采样的数据集可能偏向于只有某一类别的数据，导致某个类别的概率（频率）为0
# XGBoost
由于它计算的准而且快，所以用在各大比赛当中使用的多，不要迷信深度学习
xGBoost = extreme + GBDT = extreme + Gradient + BoostDT
Boosting -> Boosting DT -> gradient BDT -> 
使用了一阶梯度信息
## 所谓的最优函数其实是由参数决定的，其实学习方式是用最优参数。构建了模型，然后对参数求偏导。但如果没有模型参数，只有f(x)，那我们也可以只对函数本身求偏导
## 如何理解梯度提升算法？
关键在于梯度提升算法使用的不再是基于参数的负梯度，而是基于模型的**损失函数**的**负梯度**作为下次迭代中残差的近似值
$\argmin$
